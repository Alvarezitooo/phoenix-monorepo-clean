"""
âš¡ Tests de performance pour Narrative Analyzer optimisÃ©
Validation des amÃ©liorations de performance vs version originale
"""

import pytest
import asyncio
import time
from unittest.mock import AsyncMock, patch
from datetime import datetime, timezone, timedelta

from app.core.narrative_analyzer import narrative_analyzer  # Original
from app.core.narrative_analyzer_optimized import narrative_analyzer_optimized  # OptimisÃ©


class TestNarrativeAnalyzerPerformance:
    """Tests de performance comparative entre versions"""
    
    def setup_method(self):
        """Setup pour chaque test"""
        # Clear cache avant chaque test
        narrative_analyzer_optimized.cache.cache.clear()
        
        # Mock events rÃ©alistes
        self.mock_events = self._generate_realistic_events(100)
    
    def _generate_realistic_events(self, count: int) -> list:
        """GÃ©nÃ¨re des Ã©vÃ©nements rÃ©alistes pour les tests"""
        events = []
        now = datetime.now(timezone.utc)
        
        event_types = [
            "cv_generated", "cv_ats_score", "letter_sent", "mirror_match_completed",
            "energy_purchase", "onboarding_completed", "session_zero_completed"
        ]
        
        app_sources = ["cv", "letters", "journal_narratif", "luna_hub"]
        
        for i in range(count):
            # Distribution temporelle rÃ©aliste (plus d'Ã©vÃ©nements rÃ©cents)
            days_ago = int((i / count) * 30)  # Distribution sur 30 jours
            timestamp = now - timedelta(days=days_ago, hours=i % 24)
            
            event = {
                "id": f"event_{i}",
                "type": event_types[i % len(event_types)],
                "created_at": timestamp.isoformat() + "Z",
                "payload": {
                    "app_source": app_sources[i % len(app_sources)],
                    "user_action": f"action_{i}",
                    "ats_score": 75 + (i % 25),  # Scores ATS rÃ©alistes
                    "company_name": f"Company_{i}",
                    "position_title": "DÃ©veloppeur Python" if i % 3 == 0 else "Consultant",
                    "notes": "IntÃ©ressÃ© par le rÃ©seautage" if i % 5 == 0 else "Besoin d'aide quantification"
                },
                "meta": {
                    "timestamp": timestamp.isoformat() + "Z"
                }
            }
            events.append(event)
        
        return events
    
    @pytest.mark.asyncio
    async def test_context_packet_generation_performance(self):
        """
        ðŸš€ Test de performance: gÃ©nÃ©ration Context Packet
        VÃ©rifie que la version optimisÃ©e est plus rapide
        """
        
        user_id = "test_user_performance"
        
        # Mock event store pour les deux versions
        with patch('app.core.supabase_client.event_store.get_user_events', new_callable=AsyncMock) as mock_events:
            mock_events.return_value = self.mock_events
            
            # Test version originale
            start_original = time.time()
            context_original = await narrative_analyzer.generate_context_packet(user_id)
            time_original = time.time() - start_original
            
            # Test version optimisÃ©e (premier appel - pas de cache)
            start_optimized = time.time() 
            context_optimized = await narrative_analyzer_optimized.generate_context_packet(user_id)
            time_optimized = time.time() - start_optimized
            
            # Assertions qualitÃ© (rÃ©sultats similaires)
            assert context_original.user.age_days == context_optimized.user.age_days
            assert context_original.usage.session_count_7d == context_optimized.usage.session_count_7d
            assert len(context_original.usage.apps_last_7d) == len(context_optimized.usage.apps_last_7d)
            
            # Assertion performance (optimisÃ©e doit Ãªtre plus rapide)
            improvement_pct = ((time_original - time_optimized) / time_original) * 100
            
            print(f"\nâš¡ Performance Comparison:")
            print(f"   Original: {time_original:.3f}s")
            print(f"   Optimized: {time_optimized:.3f}s")
            print(f"   Improvement: {improvement_pct:.1f}%")
            
            # Version optimisÃ©e doit Ãªtre au moins 20% plus rapide
            assert time_optimized < time_original, "Version optimisÃ©e doit Ãªtre plus rapide"
            assert improvement_pct > 15, f"AmÃ©lioration insuffisante: {improvement_pct:.1f}% (attendu >15%)"
    
    @pytest.mark.asyncio
    async def test_cache_performance_boost(self):
        """
        ðŸ’¾ Test de performance: effet du cache
        VÃ©rifie que les appels suivants sont beaucoup plus rapides
        """
        
        user_id = "test_user_cache"
        
        with patch('app.core.supabase_client.event_store.get_user_events', new_callable=AsyncMock) as mock_events:
            mock_events.return_value = self.mock_events
            
            # Premier appel (pas de cache)
            start_first = time.time()
            context_first = await narrative_analyzer_optimized.generate_context_packet(user_id)
            time_first = time.time() - start_first
            
            # DeuxiÃ¨me appel (avec cache)
            start_cached = time.time()
            context_cached = await narrative_analyzer_optimized.generate_context_packet(user_id)
            time_cached = time.time() - start_cached
            
            # Cache hit doit Ãªtre beaucoup plus rapide
            cache_speedup = time_first / time_cached if time_cached > 0 else float('inf')
            
            print(f"\nðŸ’¾ Cache Performance:")
            print(f"   First call: {time_first:.3f}s")
            print(f"   Cached call: {time_cached:.3f}s")
            print(f"   Speedup: {cache_speedup:.1f}x")
            
            # RÃ©sultats identiques
            assert context_first.cache_key == context_cached.cache_key
            assert context_first.confidence == context_cached.confidence
            
            # Cache doit Ãªtre au moins 5x plus rapide
            assert time_cached < time_first / 5, "Cache doit Ãªtre significativement plus rapide"
    
    @pytest.mark.asyncio
    async def test_large_dataset_scalability(self):
        """
        ðŸ“ˆ Test de scalabilitÃ©: gros datasets
        VÃ©rifie que l'optimisation aide avec beaucoup d'Ã©vÃ©nements  
        """
        
        # Test avec dataset important
        large_events = self._generate_realistic_events(500)
        user_id = "test_user_scalability"
        
        with patch('app.core.supabase_client.event_store.get_user_events', new_callable=AsyncMock) as mock_events:
            mock_events.return_value = large_events
            
            # Test version optimisÃ©e avec gros dataset
            start_large = time.time()
            context_large = await narrative_analyzer_optimized.generate_context_packet(user_id)
            time_large = time.time() - start_large
            
            print(f"\nðŸ“ˆ Scalability Test (500 events):")
            print(f"   Processing time: {time_large:.3f}s")
            print(f"   Events analyzed: {len(large_events)}")
            print(f"   Confidence: {context_large.confidence}")
            
            # Doit rester rapide mÃªme avec beaucoup d'Ã©vÃ©nements
            assert time_large < 2.0, "Traitement doit rester sous 2 secondes mÃªme avec 500 Ã©vÃ©nements"
            assert context_large.confidence > 0, "Doit produire un contexte valide"
    
    @pytest.mark.asyncio
    async def test_concurrent_requests_performance(self):
        """
        ðŸ”„ Test de performance: requÃªtes concurrentes
        VÃ©rifie que le cache aide avec la concurrence
        """
        
        users = [f"user_concurrent_{i}" for i in range(5)]
        
        with patch('app.core.supabase_client.event_store.get_user_events', new_callable=AsyncMock) as mock_events:
            mock_events.return_value = self.mock_events
            
            # Test 5 utilisateurs en parallÃ¨le
            start_concurrent = time.time()
            
            tasks = [
                narrative_analyzer_optimized.generate_context_packet(user_id)
                for user_id in users
            ]
            
            contexts = await asyncio.gather(*tasks)
            time_concurrent = time.time() - start_concurrent
            
            print(f"\nðŸ”„ Concurrent Performance:")
            print(f"   5 users processed in: {time_concurrent:.3f}s")
            print(f"   Average per user: {time_concurrent/5:.3f}s")
            
            # Tous les contextes doivent Ãªtre valides
            assert len(contexts) == 5
            assert all(ctx.confidence >= 0 for ctx in contexts)
            
            # Traitement concurrent doit Ãªtre efficace
            assert time_concurrent < 3.0, "5 utilisateurs en parallÃ¨le doivent Ãªtre traitÃ©s en <3s"
    
    @pytest.mark.asyncio
    async def test_timestamp_parsing_optimization(self):
        """
        ðŸ“… Test d'optimisation: parsing timestamps
        VÃ©rifie l'efficacitÃ© du cache LRU pour les timestamps
        """
        
        from app.core.narrative_analyzer_optimized import parse_iso_timestamp
        
        # Clear cache
        parse_iso_timestamp.cache_clear()
        
        # Test timestamps rÃ©pÃ©titifs (simule donnÃ©es rÃ©elles)
        timestamps = [
            "2024-01-15T10:30:00Z",
            "2024-01-15T10:31:00Z", 
            "2024-01-15T10:30:00Z",  # RÃ©pÃ©tition
            "2024-01-15T11:00:00Z",
            "2024-01-15T10:30:00Z",  # RÃ©pÃ©tition
        ]
        
        # Parsing avec timing
        start_parse = time.time()
        
        for _ in range(1000):  # Simulation usage intensif
            for ts in timestamps:
                result = parse_iso_timestamp(ts)
                assert result is not None
        
        time_parse = time.time() - start_parse
        
        # VÃ©rifier statistiques cache
        cache_info = parse_iso_timestamp.cache_info()
        hit_ratio = cache_info.hits / (cache_info.hits + cache_info.misses) if cache_info.hits + cache_info.misses > 0 else 0
        
        print(f"\nðŸ“… Timestamp Parsing:")
        print(f"   5000 parses in: {time_parse:.3f}s")
        print(f"   Cache hits: {cache_info.hits}")
        print(f"   Cache misses: {cache_info.misses}")
        print(f"   Hit ratio: {hit_ratio:.2%}")
        
        assert time_parse < 0.1, "Parsing de 5000 timestamps doit Ãªtre <0.1s"
        assert hit_ratio > 0.5, "Cache LRU doit avoir >50% de hits"
    
    def test_cache_stats_available(self):
        """
        ðŸ“Š Test: statistiques du cache disponibles
        VÃ©rifie que les mÃ©triques de monitoring sont accessibles
        """
        
        stats = narrative_analyzer_optimized.get_cache_stats()
        
        assert "cache_entries" in stats
        assert "lru_cache_info" in stats
        assert isinstance(stats["cache_entries"], int)
        assert isinstance(stats["lru_cache_info"], dict)
        
        print(f"\nðŸ“Š Cache Stats: {stats}")
    
    @pytest.mark.asyncio
    async def test_memory_efficiency(self):
        """
        ðŸ§  Test d'efficacitÃ© mÃ©moire: pas de fuites
        VÃ©rifie que le cache se limite correctement
        """
        
        # GÃ©nÃ©rer beaucoup d'utilisateurs diffÃ©rents
        with patch('app.core.supabase_client.event_store.get_user_events', new_callable=AsyncMock) as mock_events:
            mock_events.return_value = self.mock_events[:50]  # Dataset plus petit pour ce test
            
            # GÃ©nÃ©rer 150 contextes (plus que la limite du cache de 100)
            for i in range(150):
                user_id = f"memory_test_user_{i}"
                await narrative_analyzer_optimized.generate_context_packet(user_id)
            
            # VÃ©rifier que le cache ne dÃ©passe pas la limite
            cache_size = len(narrative_analyzer_optimized.cache.cache)
            
            print(f"\nðŸ§  Memory Efficiency:")
            print(f"   Cache size after 150 users: {cache_size}")
            
            assert cache_size <= 100, "Cache ne doit pas dÃ©passer la limite configurÃ©e"