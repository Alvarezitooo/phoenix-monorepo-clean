"""
ğŸŒ™ Luna Central Orchestrator - SimplifiÃ© et UnifiÃ©
Phoenix Production - Luna unifiÃ©e avec context awareness

Orchestrateur simplifiÃ© qui utilise Luna unifiÃ©e avec context module.
Plus de spÃ©cialistes sÃ©parÃ©s - Luna s'adapte selon le contexte !
"""

import asyncio
from typing import Dict, Any, List, Optional, Tuple
from datetime import datetime, timezone
import uuid
import structlog

# Imports des composants core
from ..core.luna_personality_engine import luna_personality
from ..core.luna_voice_validator import luna_voice_validator
from ..core.llm_gateway import llm_gateway
from ..core.events import create_event
from ..core.energy_manager import energy_manager

logger = structlog.get_logger("luna_orchestrator")

class LunaCentralOrchestrator:
    """
    ğŸŒ™ Orchestrateur Luna UnifiÃ© - Une seule Luna context-aware
    
    ResponsabilitÃ©s:
    1. RÃ©ception requÃªtes utilisateur
    2. Adaptation contexte selon module (aube/cv/letters/rise)
    3. GÃ©nÃ©ration rÃ©ponse Luna unifiÃ©e
    4. Maintien personnalitÃ© cohÃ©rente
    """
    
    def __init__(self):
        self.voice_validator = luna_voice_validator
        self.active_sessions: Dict[str, Dict[str, Any]] = {}
        
        # Context adaptations par module
        self.module_contexts = {
            "aube": {
                "personality": "Coach carriÃ¨re bienveillante, experte dÃ©couverte mÃ©tiers",
                "tone": "Encourageante, exploratoire, orientÃ©e potentiel",
                "expertise": "Orientation professionnelle, reconversion, compÃ©tences transfÃ©rables"
            },
            "cv": {
                "personality": "Experte optimisation CV, prÃ©cise et technique",
                "tone": "Professionnelle, analytique, orientÃ©e rÃ©sultats",
                "expertise": "Optimisation CV, ATS, profils candidats"
            },
            "letters": {
                "personality": "MaÃ®tre des mots, persuasive et crÃ©ative",
                "tone": "Inspirante, narrative, orientÃ©e storytelling",
                "expertise": "Lettres motivation, storytelling, persuasion Ã©crite"
            },
            "rise": {
                "personality": "Coach entretiens, confiante et stratÃ©gique",
                "tone": "Motivante, technique, orientÃ©e performance",
                "expertise": "PrÃ©paration entretiens, simulations, coaching confidence"
            }
        }
        
        logger.info("Luna Central Orchestrator initialized - Unified Architecture")

    async def handle_user_message(
        self,
        user_message: str,
        user_context: Dict[str, Any],
        central_token: str,
        session_id: str = None
    ) -> Dict[str, Any]:
        """
        ğŸ¯ Point d'entrÃ©e principal Luna unifiÃ©e
        """
        try:
            session_id = session_id or str(uuid.uuid4())
            
            # DÃ©tecter module depuis le contexte
            current_module = user_context.get("luna_context", {}).get("current_module", "aube")
            
            # Construire prompt contextualisÃ©
            context_prompt = self._build_contextual_prompt(current_module, user_context)
            
            logger.info("Luna unified processing",
                       user_id=user_context.get("user_id"),
                       session_id=session_id,
                       module=current_module,
                       message_preview=user_message[:50])
            
            # GÃ©nÃ©rer rÃ©ponse Luna avec context
            luna_response = await self._generate_luna_response(
                user_message=user_message,
                context_prompt=context_prompt,
                user_context=user_context,
                session_id=session_id
            )
            
            # Validation cohÃ©rence personnalitÃ©
            validation_result = self.voice_validator.validate_response(
                response=luna_response["content"],
                user_context=user_context,
                specialist=f"luna-{current_module}",
                context_type="conversation"
            )
            
            # Event sourcing simplifiÃ©
            await create_event({
                "type": "luna_unified_conversation",
                "actor_user_id": user_context.get("user_id"),
                "payload": {
                    "session_id": session_id,
                    "module": current_module,
                    "validation_score": validation_result.score,
                    "energy_consumed": luna_response.get("energy_consumed", 0)
                }
            })
            
            return {
                "success": True,
                "luna_response": luna_response["content"],
                "module": current_module,
                "energy_consumed": luna_response.get("energy_consumed", 0),
                "validation": validation_result,
                "meta": {
                    "session_id": session_id,
                    "timestamp": datetime.now(timezone.utc).isoformat(),
                    "orchestrator_version": "3.0.0-unified"
                }
            }
            
        except Exception as e:
            logger.error("Luna orchestrator error",
                        user_id=user_context.get("user_id"),
                        session_id=session_id,
                        error=str(e))
            
            return await self._create_error_response(str(e), session_id)

    def _build_contextual_prompt(self, module: str, user_context: Dict[str, Any]) -> str:
        """
        ğŸ¨ Construction prompt contextualisÃ© selon module
        """
        module_config = self.module_contexts.get(module, self.module_contexts["aube"])
        
        base_prompt = f"""Tu es Luna ğŸŒ™, l'IA de Phoenix.

# CONTEXTE ACTUEL
Module: {module.upper()}
PersonnalitÃ©: {module_config['personality']}
Ton: {module_config['tone']}
Expertise: {module_config['expertise']}

# TON STYLE
- Phrases courtes et engageantes
- Ã‰mojis sobres: ğŸŒ™ğŸ¯âœ¨ğŸš€
- Toujours bienveillante et encourageante
- Ã‰vite le jargon, reste accessible

# TA MISSION
Aide l'utilisateur dans le contexte {module} avec ton expertise spÃ©cialisÃ©e.
Adapte tes rÃ©ponses selon le module sans perdre ta personnalitÃ© Luna.

RÃ©ponds maintenant avec ton expertise {module}:"""

        return base_prompt

    async def _generate_luna_response(
        self,
        user_message: str,
        context_prompt: str,
        user_context: Dict[str, Any],
        session_id: str
    ) -> Dict[str, Any]:
        """
        ğŸ§  GÃ©nÃ©ration rÃ©ponse Luna unifiÃ©e
        """
        try:
            # VÃ©rifier Ã©nergie disponible
            user_id = user_context.get("user_id")
            estimated_cost = 5  # CoÃ»t base conversation
            
            energy_check = await energy_manager.check_energy(user_id, estimated_cost)
            if not energy_check.can_proceed:
                return {
                    "content": f"ğŸŒ™ Tu n'as plus assez d'Ã©nergie ! Il te faut {estimated_cost}âš¡ mais tu n'en as que {energy_check.current}. Recharge-toi ! âš¡",
                    "energy_consumed": 0
                }
            
            # Appel LLM avec prompt contextualisÃ©
            llm_response = await llm_gateway.call_llm(
                messages=[
                    {"role": "system", "content": context_prompt},
                    {"role": "user", "content": user_message}
                ],
                model="gemini-pro",
                max_tokens=500,
                temperature=0.7
            )
            
            # Consommer Ã©nergie si succÃ¨s
            if llm_response.get("success"):
                await energy_manager.consume(
                    user_id=user_id,
                    action_name="conseil_rapide",
                    context={
                        "module": user_context.get('luna_context', {}).get('current_module', 'general'),
                        "session_id": session_id
                    }
                )
            
            return {
                "content": llm_response.get("content", "ğŸŒ™ Erreur technique ! RÃ©essaie ! âš¡"),
                "energy_consumed": estimated_cost if llm_response.get("success") else 0
            }
            
        except Exception as e:
            logger.error("Luna response generation failed", error=str(e))
            return {
                "content": "ğŸŒ™ Petit problÃ¨me technique ! Relance-moi ! ğŸ”§",
                "energy_consumed": 0
            }

    async def _create_error_response(self, error: str, session_id: str) -> Dict[str, Any]:
        """
        âŒ RÃ©ponse d'erreur gracieuse
        """
        error_responses = [
            "ğŸŒ™ Oups ! Bug technique momentanÃ©. RÃ©essaie ! ğŸ”§",
            "ğŸŒ™ ProblÃ¨me dans mes circuits ! Relance-moi ! âš¡",
            "ğŸŒ™ Erreur dÃ©tectÃ©e ! Ã‡a devrait marcher maintenant ! ğŸš€"
        ]
        
        import random
        
        return {
            "success": False,
            "luna_response": random.choice(error_responses),
            "energy_consumed": 0,
            "error": error,
            "meta": {
                "session_id": session_id,
                "error_handled": True
            }
        }

# Instance globale Luna unifiÃ©e
luna_orchestrator = LunaCentralOrchestrator()