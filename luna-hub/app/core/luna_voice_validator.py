"""
ğŸ­ Luna Voice Consistency Validator
Phoenix Production - Enterprise Quality

Validation que chaque rÃ©ponse respecte strictement l'ADN Luna.
SystÃ¨me de scoring et correction automatique.
"""

from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass
from enum import Enum
import re

@dataclass
class ValidationResult:
    """RÃ©sultat de validation d'une rÃ©ponse Luna"""
    is_valid: bool
    score: float
    passed_checks: List[str]
    failed_checks: List[str]
    suggestions: List[str]
    corrected_response: Optional[str] = None

class ValidationSeverity(Enum):
    """Niveaux de sÃ©vÃ©ritÃ© pour la validation"""
    STRICT = 0.9      # Production - 90% des critÃ¨res
    STANDARD = 0.7    # DÃ©veloppement - 70% des critÃ¨res  
    PERMISSIVE = 0.5  # Tests - 50% des critÃ¨res

class LunaVoiceValidator:
    """
    ğŸ­ Validateur de cohÃ©rence vocale Luna
    
    S'assure que chaque rÃ©ponse gÃ©nÃ©rÃ©e respecte l'ADN Luna dÃ©fini
    dans luna_personality_engine.py
    """
    
    # ğŸ¯ CritÃ¨res de validation avec poids
    VALIDATION_CRITERIA = {
        "has_enthusiasm": {
            "weight": 0.2,
            "description": "PrÃ©sence de mots enthousiastes",
            "keywords": ["super", "excellent", "parfait", "gÃ©nial", "top", "bravo", "nickel"]
        },
        
        "has_luna_signature": {
            "weight": 0.15, 
            "description": "PrÃ©sence emojis/expressions Luna",
            "patterns": ["ğŸŒ™", "âœ¨", "ğŸš€", "ğŸ’ª", "ğŸ¯", "âš¡", "Luna"]
        },
        
        "positive_vocabulary": {
            "weight": 0.2,
            "description": "Vocabulaire positif",
            "forbidden": ["compliquÃ©", "difficile", "impossible", "problÃ¨me", "Ã©chec", "nul"],
            "preferred": ["dÃ©fi", "intÃ©ressant", "ambitieux", "apprentissage", "opportunitÃ©"]
        },
        
        "encouragement_present": {
            "weight": 0.15,
            "description": "PrÃ©sence d'encouragement",
            "patterns": ["tu peux", "on y va", "ensemble", "je suis lÃ ", "tu gÃ¨res", "on va y arriver"]
        },
        
        "user_reference": {
            "weight": 0.1,
            "description": "RÃ©fÃ©rence personnalisÃ©e Ã  l'utilisateur",
            "optional": True  # Pas toujours nÃ©cessaire
        },
        
        "appropriate_length": {
            "weight": 0.1,
            "description": "Longueur appropriÃ©e",
            "min_chars": 30,
            "max_chars": 800
        },
        
        "conversational_tone": {
            "weight": 0.1,
            "description": "Ton conversationnel",
            "indicators": ["?", "!", "on", "tu", "nous", "ensemble"]
        }
    }

    def __init__(self, severity: ValidationSeverity = ValidationSeverity.STANDARD):
        self.severity = severity
        self.threshold = severity.value

    def validate_response(self, response: str, user_context: Dict = None, specialist: str = None, context_type: str = None) -> ValidationResult:
        """
        ğŸ” Validation complÃ¨te d'une rÃ©ponse Luna
        
        Args:
            response: RÃ©ponse Ã  valider
            user_context: Contexte utilisateur (optionnel)
            specialist: SpÃ©cialiste origine (optionnel)
            context_type: Type de contexte (optionnel)
            
        Returns:
            ValidationResult avec score et suggestions
        """
        if not response or not response.strip():
            return ValidationResult(
                is_valid=False,
                score=0.0,
                passed_checks=[],
                failed_checks=["empty_response"],
                suggestions=["La rÃ©ponse ne peut pas Ãªtre vide"]
            )

        # ExÃ©cuter tous les critÃ¨res
        check_results = {}
        total_score = 0.0
        max_possible_score = 0.0

        for criterion_name, criterion_config in self.VALIDATION_CRITERIA.items():
            weight = criterion_config["weight"]
            max_possible_score += weight
            
            if criterion_config.get("optional", False) and not user_context:
                continue  # Skip les critÃ¨res optionnels sans contexte
                
            check_result = self._check_criterion(response, criterion_name, criterion_config, user_context)
            check_results[criterion_name] = check_result
            
            if check_result["passed"]:
                total_score += weight

        # Calculer score final
        final_score = total_score / max_possible_score if max_possible_score > 0 else 0.0
        is_valid = final_score >= self.threshold

        # SÃ©parer passed/failed
        passed_checks = [name for name, result in check_results.items() if result["passed"]]
        failed_checks = [name for name, result in check_results.items() if not result["passed"]]

        # GÃ©nÃ©rer suggestions
        suggestions = self._generate_suggestions(failed_checks, check_results, response)

        # GÃ©nÃ©rer correction automatique si nÃ©cessaire
        corrected_response = None
        if not is_valid and self.severity != ValidationSeverity.PERMISSIVE:
            corrected_response = self._auto_correct_response(response, failed_checks, user_context)

        return ValidationResult(
            is_valid=is_valid,
            score=final_score,
            passed_checks=passed_checks,
            failed_checks=failed_checks,
            suggestions=suggestions,
            corrected_response=corrected_response
        )

    def _check_criterion(self, response: str, criterion_name: str, config: Dict, user_context: Dict = None) -> Dict:
        """VÃ©rification d'un critÃ¨re spÃ©cifique"""
        response_lower = response.lower()
        
        if criterion_name == "has_enthusiasm":
            keywords = config["keywords"]
            found = [kw for kw in keywords if kw in response_lower]
            return {
                "passed": len(found) > 0,
                "details": f"Mots trouvÃ©s: {found}" if found else "Aucun mot enthousiaste trouvÃ©",
                "found_items": found
            }
            
        elif criterion_name == "has_luna_signature":
            patterns = config["patterns"]
            found = [p for p in patterns if p.lower() in response_lower or p in response]
            return {
                "passed": len(found) > 0,
                "details": f"Signatures trouvÃ©es: {found}" if found else "Aucune signature Luna",
                "found_items": found
            }
            
        elif criterion_name == "positive_vocabulary":
            forbidden = config["forbidden"]
            found_negative = [word for word in forbidden if word in response_lower]
            return {
                "passed": len(found_negative) == 0,
                "details": f"Mots nÃ©gatifs trouvÃ©s: {found_negative}" if found_negative else "Vocabulaire positif âœ…",
                "found_negative": found_negative
            }
            
        elif criterion_name == "encouragement_present":
            patterns = config["patterns"]
            found = [p for p in patterns if p in response_lower]
            return {
                "passed": len(found) > 0,
                "details": f"Encouragements: {found}" if found else "Manque d'encouragement",
                "found_items": found
            }
            
        elif criterion_name == "user_reference":
            if not user_context:
                return {"passed": True, "details": "Pas de contexte utilisateur"}
            
            user_name = user_context.get("name") or ""
            if not user_name:
                return {"passed": True, "details": "Pas de nom utilisateur"}
            user_name = user_name.lower()
                
            has_reference = user_name in response_lower
            return {
                "passed": has_reference,
                "details": f"RÃ©fÃ©rence Ã  '{user_name}': {'âœ…' if has_reference else 'âŒ'}"
            }
            
        elif criterion_name == "appropriate_length":
            length = len(response)
            min_chars = config["min_chars"]
            max_chars = config["max_chars"]
            is_appropriate = min_chars <= length <= max_chars
            return {
                "passed": is_appropriate,
                "details": f"Longueur: {length} chars (min: {min_chars}, max: {max_chars})",
                "actual_length": length
            }
            
        elif criterion_name == "conversational_tone":
            indicators = config["indicators"]
            found = [ind for ind in indicators if ind in response_lower]
            has_conversation_markers = len(found) >= 2  # Au moins 2 marqueurs
            return {
                "passed": has_conversation_markers,
                "details": f"Marqueurs conversationnels: {found}",
                "found_items": found
            }
            
        return {"passed": False, "details": "CritÃ¨re non implÃ©mentÃ©"}

    def _generate_suggestions(self, failed_checks: List[str], check_results: Dict, response: str) -> List[str]:
        """GÃ©nÃ©ration de suggestions d'amÃ©lioration"""
        suggestions = []
        
        if "has_enthusiasm" in failed_checks:
            suggestions.append("â• Ajouter des mots enthousiastes: 'super', 'excellent', 'parfait'")
            
        if "has_luna_signature" in failed_checks:
            suggestions.append("ğŸŒ™ Ajouter l'emoji Luna ou mentionner 'Luna'")
            
        if "positive_vocabulary" in failed_checks:
            negative_words = check_results.get("positive_vocabulary", {}).get("found_negative", [])
            suggestions.append(f"ğŸ”„ Remplacer mots nÃ©gatifs: {', '.join(negative_words)} par alternatives positives")
            
        if "encouragement_present" in failed_checks:
            suggestions.append("ğŸ’ª Ajouter encouragement: 'tu peux', 'on y va', 'ensemble'")
            
        if "appropriate_length" in failed_checks:
            actual_length = check_results.get("appropriate_length", {}).get("actual_length", 0)
            if actual_length < 30:
                suggestions.append("ğŸ“ Ã‰toffer la rÃ©ponse (trop courte)")
            else:
                suggestions.append("âœ‚ï¸ Raccourcir la rÃ©ponse (trop longue)")
                
        if "conversational_tone" in failed_checks:
            suggestions.append("ğŸ’¬ Rendre plus conversationnel: questions, exclamations, 'tu', 'on'")

        return suggestions

    def _auto_correct_response(self, response: str, failed_checks: List[str], user_context: Dict = None) -> str:
        """Correction automatique basique de la rÃ©ponse"""
        corrected = response
        
        # Ajouter emoji Luna si manquant
        if "has_luna_signature" in failed_checks and "ğŸŒ™" not in corrected:
            if "Luna" in corrected:
                corrected = corrected.replace("Luna", "Luna ğŸŒ™", 1)
            else:
                corrected = "ğŸŒ™ " + corrected
                
        # Ajouter encouragement si manquant
        if "encouragement_present" in failed_checks:
            if not corrected.endswith("!"):
                corrected += " Tu vas y arriver ! ğŸ’ª"
            else:
                corrected += " ğŸ’ª"
                
        # Remplacer vocabulaire nÃ©gatif
        if "positive_vocabulary" in failed_checks:
            replacements = {
                "compliquÃ©": "intÃ©ressant",
                "difficile": "challengeant", 
                "impossible": "ambitieux",
                "problÃ¨me": "dÃ©fi"
            }
            for negative, positive in replacements.items():
                corrected = re.sub(rf'\b{negative}\b', positive, corrected, flags=re.IGNORECASE)

        # Ajouter enthousiasme si manquant
        if "has_enthusiasm" in failed_checks:
            if not any(word in corrected.lower() for word in ["super", "excellent", "parfait"]):
                corrected = "Super ! " + corrected
                
        return corrected

    def get_validation_report(self, validation_result: ValidationResult) -> str:
        """
        ğŸ“Š GÃ©nÃ¨re un rapport de validation dÃ©taillÃ©
        """
        status = "âœ… VALIDÃ‰" if validation_result.is_valid else "âŒ Ã‰CHEC"
        score_pct = f"{validation_result.score:.1%}"
        
        report_lines = [
            f"ğŸ­ VALIDATION LUNA VOICE",
            f"Status: {status} | Score: {score_pct}",
            f"Seuil: {self.threshold:.1%} | CritÃ¨res passÃ©s: {len(validation_result.passed_checks)}/{len(validation_result.passed_checks) + len(validation_result.failed_checks)}",
            ""
        ]
        
        if validation_result.passed_checks:
            report_lines.append("âœ… CRITÃˆRES RESPECTÃ‰S:")
            for check in validation_result.passed_checks:
                report_lines.append(f"  â€¢ {check}")
            report_lines.append("")
            
        if validation_result.failed_checks:
            report_lines.append("âŒ CRITÃˆRES Ã‰CHOUÃ‰S:")
            for check in validation_result.failed_checks:
                report_lines.append(f"  â€¢ {check}")
            report_lines.append("")
            
        if validation_result.suggestions:
            report_lines.append("ğŸ’¡ SUGGESTIONS:")
            for suggestion in validation_result.suggestions:
                report_lines.append(f"  {suggestion}")
                
        return "\n".join(report_lines)

# Instance globale pour import direct
luna_voice_validator = LunaVoiceValidator()