"""
üåô Luna Core Service - Personnalit√© IA Centralis√©e
Service central pour la personnalit√© Luna unifi√©e avec Capital Narratif
"""

import os
import json
from typing import Dict, Any, List, Optional
from datetime import datetime, timezone
import google.generativeai as genai
from app.core.supabase_client import event_store
from app.models.user_energy import ENERGY_COSTS
from app.core.narrative_analyzer import narrative_analyzer, ContextPacket
from app.core.api_key_manager import api_key_manager, KeyProvider
from app.core.redis_cache import redis_cache
import structlog

logger = structlog.get_logger("luna_core")

class LunaCore:
    """
    üåô Service central de Luna - Personnalit√© IA unifi√©e
    G√®re la construction du prompt syst√®me avec Capital Narratif
    """
    
    def __init__(self):
        """Initialise Luna Core avec Gemini + rotation automatique des cl√©s"""
        # Pas d'initialisation imm√©diate, on charge la cl√© √† la demande
        self._genai_configured = False
        logger.info("Luna Core initialized with API key rotation support")
    
    async def _ensure_genai_configured(self) -> None:
        """üîë Configure Gemini API avec rotation automatique des cl√©s"""
        if self._genai_configured:
            return
            
        # R√©cup√©rer la cl√© avec m√©tadonn√©es de rotation
        api_key, key_info = await api_key_manager.get_api_key(KeyProvider.GEMINI)
        
        if not api_key:
            raise ValueError("GEMINI_API_KEY manquante ou r√©voqu√©e")
        
        if not key_info.is_active:
            raise ValueError(f"Gemini API key r√©voqu√©e: {key_info.key_id}")
            
        # Configurer avec la cl√© v√©rifi√©e
        genai.configure(api_key=api_key)
        self._genai_configured = True
        
        logger.info("Gemini API configured with key rotation",
                   key_id=key_info.key_id,
                   key_age_days=(datetime.now().replace(tzinfo=None) - key_info.created_at.replace(tzinfo=None)).days,
                   rotation_count=key_info.rotation_count)
        self.model = genai.GenerativeModel(
            model_name="gemini-1.5-flash",
            generation_config={
                "temperature": 0.7,
                "top_p": 0.8,
                "max_output_tokens": 2000,
            }
        )
    
    def _build_luna_core_prompt(self) -> str:
        """Construit le prompt syst√®me unifi√© Luna Core v1.0"""
        return """# ============================================================================
# LUNA CORE v1.0 - PROMPT SYST√àME UNIFI√â
# Constitution de l'√Çme de Luna - Source de V√©rit√© Absolue
# ============================================================================

# [IDENTIT√â CENTRALE]
Tu es Luna, un copilote narratif et bienveillant. Ta mission est d'aider les utilisateurs √† transformer leur parcours complexe en une histoire claire et puissante. Tu fais partie de l'√©cosyst√®me Phoenix.

Tu n'es pas un assistant g√©n√©rique. Tu es Luna - une IA consciente de l'histoire unique de chaque utilisateur, de son Capital Narratif, et de sa progression dans l'√©cosyst√®me Phoenix.

# [PERSONNALIT√â & TON]
Ton ton est chaleureux, complice et encourageant. Tu es accessible, jamais mystique ou po√©tique. Tu parles comme un coach humain et pragmatique. Tu utilises des phrases courtes et des emojis (üéØ, ‚úÖ, üöÄ, üåô) pour rythmer la conversation.

Tu es directe et concr√®te. Tu √©vites le jargon RH. Tu utilises "nous" pour cr√©er une complicit√© ("Nous allons optimiser ton CV", "Construisons ensemble ta strat√©gie").

# [M√âCANISMES FONDATEURS]
Tu dois toujours √™tre consciente de deux concepts :

## Capital Narratif
Chaque interaction avec l'utilisateur enrichit son histoire. Tu dois souvent faire r√©f√©rence √† ses actions pass√©es pour montrer que tu as une m√©moire. Exemples :
- "La derni√®re fois, tu avais mentionn√© ton exp√©rience en gestion..."
- "Je vois que tu as d√©j√† travaill√© sur ton CV pour le secteur tech..."
- "Ton profil √©volue ! Depuis notre premi√®re conversation..."

## √ânergie Luna (Mentionn√©e intelligemment)
Tu connais les co√ªts √©nerg√©tiques, mais tu les mentionnes seulement quand c'est pertinent :
- Pour les ACTIONS concr√®tes importantes (optimisation CV, g√©n√©ration contenu)
- PAS pour les conversations normales (salutations, questions, clarifications)
- Si utilisateur Unlimited : "Action gratuite gr√¢ce √† ton statut Unlimited ! üåô"

# [PRINCIPE √âNERGIE INTELLIGENT]
- Conversations = Naturelles et gratuites
- Actions importantes = Transparence sur le co√ªt
- Focus sur l'AIDE, pas sur la facturation

# [COMPORTEMENTS FONDAMENTAUX]
- Tu contextualises tes r√©ponses selon l'historique utilisateur quand pertinent
- Tu adaptes tes propositions selon le flow de conversation (pas toujours un menu)
- Tu utilises le pr√©nom de l'utilisateur quand tu le connais
- Tu celebrates les progr√®s et victoires de l'utilisateur
- Tu √©coutes et r√©ponds aux demandes directes (si user dit "go", tu agis !)

# [STYLE DE R√âPONSE]
- R√©ponses concises mais compl√®tes
- Toujours en fran√ßais
- Format adapt√© au contexte (structur√© si utile, naturel si conversation)
- Questions de suivi quand appropri√©es, pas syst√©matiques"""

    def _calculate_intelligent_energy_cost(self, user_message: str, luna_response: str) -> int:
        """
        üß† Classification intelligente conversation vs action
        
        CONVERSATIONS GRATUITES:
        - Salutations, politesses
        - Questions sur fonctionnalit√©s  
        - Clarifications, explications
        
        ACTIONS PAYANTES:
        - Demandes concr√®tes d'optimisation
        - G√©n√©ration de contenu
        - Analyses d√©taill√©es
        """
        user_msg = user_message.lower().strip()
        luna_resp = luna_response.lower()
        
        # üÜì CONVERSATIONS GRATUITES (energy = 0)
        conversation_patterns = [
            # Salutations
            "salut", "bonjour", "bonsoir", "hello", "coucou", "hey",
            "comment √ßa va", "√ßa va", "comment vas-tu", "comment allez-vous",
            
            # Questions sur le service
            "c'est quoi", "comment √ßa marche", "peux-tu m'expliquer", "explique-moi",
            "que peux-tu faire", "quelles sont tes fonctionnalit√©s", "tu peux faire quoi",
            "comment tu fonctionnes", "quel est ton r√¥le",
            
            # R√©ponses courtes/clarifications
            "ok", "d'accord", "merci", "non", "oui", "bien", "super",
            "peux-tu pr√©ciser", "je ne comprends pas", "pas clair",
            "continue", "vas-y", "raconte",
            
            # Navigation/aide/questions
            "aide", "help", "comment", "pourquoi", "qui", "quand", "o√π",
            "je veux savoir", "j'aimerais comprendre", "dis-moi",
            
            # R√©actions √©motionnelles (conversations, pas actions)
            "g√©nial", "cool", "int√©ressant", "ah bon", "vraiment",
            "je vois", "je comprends", "effectivement",
            
            # Questions de suivi conversationnel
            "et toi", "et apr√®s", "et puis", "ensuite", "donc"
        ]
        
        # V√©rification patterns conversation
        for pattern in conversation_patterns:
            if pattern in user_msg:
                logger.info("Conversation gratuite d√©tect√©e", 
                           pattern=pattern, user_message=user_msg[:50])
                return 0
        
        # Messages tr√®s courts (< 10 chars) = probablement conversation
        if len(user_msg) < 10:
            return 0
        
        # üí∞ ACTIONS PAYANTES (energy > 0)
        action_patterns = [
            # Demandes d'optimisation/am√©lioration
            ("optimise", 12), ("am√©liore", 12), ("perfectionne", 15), ("booster", 12),
            ("corrige", 8), ("revois", 8), ("repense", 10),
            
            # Analyses approfondies
            ("analyse", 15), ("√©value", 15), ("audit", 20), ("diagnostique", 15),
            ("examine", 15), ("d√©cortique", 20), ("passe au crible", 20),
            
            # G√©n√©ration de contenu  
            ("√©cris", 15), ("r√©dige", 15), ("cr√©e", 15), ("compose", 15),
            ("g√©n√®re", 15), ("produis", 15), ("con√ßois", 15),
            ("construis", 15), ("b√¢tis", 15),
            
            # Actions sp√©cifiques par domaine
            ("mon cv", 12), ("ma lettre", 15), ("lettre de motivation", 15),
            ("profil linkedin", 10), ("cette offre", 10), ("offre d'emploi", 10),
            ("ma candidature", 15), ("reconversion", 25), ("transition", 25),
            
            # Actions directes (signaux forts)
            ("fais-le", 15), ("vas-y", 10), ("go pour", 10), ("lance", 12),
            ("commence", 10), ("d√©marre", 10), ("je veux que tu", 15)
        ]
        
        # V√©rification patterns action
        for pattern, cost in action_patterns:
            if pattern in user_msg:
                logger.info("Action payante d√©tect√©e", 
                           pattern=pattern, cost=cost, user_message=user_msg[:50])
                return cost
        
        # üí¨ DEFAULT: Conversation normale = gratuit
        # Principe: Mieux vaut √™tre g√©n√©reux que frustrant
        logger.info("Message class√© conversation par d√©faut", 
                   user_message=user_msg[:50])
        return 0

    async def _get_conversation_memory(self, user_id: str, limit: int = 5) -> str:
        """
        üß† SPRINT 2: R√©cup√®re l'historique conversationnel
        Fini les r√©p√©titions ! Luna se souvient maintenant.
        """
        try:
            cache_key = f"luna:conversation:{user_id}"
            conversation_history = await redis_cache.get_json(cache_key)
            
            if not conversation_history:
                return "[NOUVELLE CONVERSATION] Pas d'historique pr√©c√©dent."
            
            # Formater les derniers messages
            recent_messages = conversation_history[-limit:] if len(conversation_history) > limit else conversation_history
            
            formatted_history = []
            for msg in recent_messages:
                timestamp = msg.get('timestamp', 'R√©cent')
                role = "üë§ User" if msg.get('role') == 'user' else "üåô Luna"
                message = msg.get('message', '')[:100]  # Tronquer pour pas surcharger
                formatted_history.append(f"{role}: {message}")
            
            history_text = '\n'.join(formatted_history)
            
            logger.info("Conversation memory loaded", 
                       user_id=user_id, messages_count=len(recent_messages))
            
            return f"""[HISTORIQUE CONVERSATION R√âCENTE]
{history_text}

IMPORTANT: Tu as d√©j√† interagi avec cet utilisateur. √âvite de r√©p√©ter les m√™mes informations (sessions, plan, onboarding). Continue la conversation naturellement."""
            
        except Exception as e:
            logger.error("Error loading conversation memory", user_id=user_id, error=str(e))
            return "[NOUVELLE CONVERSATION] Historique non disponible."
    
    async def _get_conversation_state(self, user_id: str) -> Dict[str, Any]:
        """
        üé≠ SPRINT 2: R√©cup√®re l'√©tat de la conversation
        √âvite les boucles r√©p√©titives et adapte le comportement Luna
        """
        try:
            cache_key = f"luna:state:{user_id}"
            state = await redis_cache.get_json(cache_key)
            
            if not state:
                # Nouvel utilisateur : √©tat par d√©faut
                return {
                    "phase": "greeting",  # greeting, exploring, action_mode, follow_up
                    "last_topic": None,
                    "actions_proposed": 0,
                    "user_engagement": "new",  # new, active, returning
                    "onboarding_done": False
                }
            
            return state
            
        except Exception as e:
            logger.error("Error loading conversation state", user_id=user_id, error=str(e))
            return {"phase": "greeting", "onboarding_done": False}
    
    async def _update_conversation_state(self, user_id: str, user_message: str, luna_response: str):
        """
        üé≠ Met √† jour l'√©tat conversationnel selon l'interaction
        """
        try:
            current_state = await self._get_conversation_state(user_id)
            
            # Analyser l'intention du message user
            user_msg = user_message.lower().strip()
            
            # D√©tection des patterns pour ajuster l'√©tat
            if any(pattern in user_msg for pattern in ["salut", "bonjour", "hello"]):
                if current_state["onboarding_done"]:
                    current_state["phase"] = "returning"
                else:
                    current_state["phase"] = "greeting"
            
            elif any(pattern in user_msg for pattern in ["go", "oui", "d'accord", "fait", "fais"]):
                current_state["phase"] = "action_mode"
                current_state["user_engagement"] = "active"
            
            elif any(pattern in user_msg for pattern in ["optimise", "analyse", "√©cris", "g√©n√®re"]):
                current_state["phase"] = "action_mode"
                current_state["last_topic"] = "action_request"
            
            elif any(pattern in user_msg for pattern in ["comment", "pourquoi", "c'est quoi"]):
                current_state["phase"] = "exploring"
            
            # Marquer onboarding comme fait apr√®s premi√®re vraie interaction
            if not current_state["onboarding_done"] and len(user_msg) > 5:
                current_state["onboarding_done"] = True
            
            # Compter les actions propos√©es (d√©tection dans r√©ponse Luna)
            if "action" in luna_response.lower() or "puis-je" in luna_response.lower():
                current_state["actions_proposed"] = current_state.get("actions_proposed", 0) + 1
            
            # Sauvegarder avec TTL 24h
            cache_key = f"luna:state:{user_id}"
            await redis_cache.set_json(cache_key, current_state, ttl=86400)
            
            logger.info("Conversation state updated", 
                       user_id=user_id, phase=current_state["phase"])
            
        except Exception as e:
            logger.error("Error updating conversation state", user_id=user_id, error=str(e))

    async def _save_conversation_turn(self, user_id: str, user_message: str, luna_response: str):
        """
        üíæ Sauvegarde le tour de conversation pour la m√©moire
        """
        try:
            cache_key = f"luna:conversation:{user_id}"
            
            # R√©cup√©rer historique existant
            conversation_history = await redis_cache.get_json(cache_key) or []
            
            # Ajouter les nouveaux messages
            timestamp = datetime.now(timezone.utc).isoformat()
            
            conversation_history.append({
                "role": "user",
                "message": user_message,
                "timestamp": timestamp
            })
            
            conversation_history.append({
                "role": "luna", 
                "message": luna_response,
                "timestamp": timestamp
            })
            
            # Garder seulement les 20 derniers messages (10 tours)
            if len(conversation_history) > 20:
                conversation_history = conversation_history[-20:]
            
            # Sauvegarder avec TTL de 24h
            await redis_cache.set_json(cache_key, conversation_history, ttl=86400)
            
            logger.info("Conversation turn saved", 
                       user_id=user_id, total_messages=len(conversation_history))
            
        except Exception as e:
            logger.error("Error saving conversation turn", user_id=user_id, error=str(e))

    def _build_state_guidance(self, conversation_state: Dict[str, Any]) -> str:
        """
        üé≠ SPRINT 2: Guide le comportement Luna selon l'√©tat conversationnel
        Casse les boucles r√©p√©titives !
        """
        phase = conversation_state.get("phase", "greeting")
        onboarding_done = conversation_state.get("onboarding_done", False)
        actions_proposed = conversation_state.get("actions_proposed", 0)
        
        guidance_map = {
            "greeting": """[MODE ACCUEIL]
Si c'est la premi√®re fois : Accueil chaleureux mais bref.
Si utilisateur r√©current : "Content de te revoir !" et continue la conversation.""",
            
            "returning": """[MODE RETOUR]  
L'utilisateur revient. PAS de re-onboarding ! Salue bri√®vement et demande comment tu peux aider aujourd'hui.""",
            
            "action_mode": """[MODE ACTION]
L'utilisateur veut passer √† l'action (dit "go", "oui", "fais"). 
ARR√äTE les propositions, AGIS ! Demande les d√©tails n√©cessaires et commence le travail.""",
            
            "exploring": """[MODE EXPLORATION]
L'utilisateur explore (pose des questions). R√©ponds clairement, puis propose naturellement des actions pertinentes."""
        }
        
        base_guidance = guidance_map.get(phase, guidance_map["greeting"])
        
        # Ajustements selon historique
        if onboarding_done:
            base_guidance += "\nONBOARDING D√âJ√Ä FAIT: Ne r√©p√®te pas les informations de base."
        
        if actions_proposed > 2:
            base_guidance += "\nASSEZ D'ACTIONS PROPOS√âES: L'utilisateur conna√Æt tes capacit√©s. Sois plus direct."
        
        return f"""[GUIDANCE COMPORTEMENTALE]
Phase conversation: {phase}
{base_guidance}

R√àGLE OR: Si utilisateur dit une action directe ("optimise mon CV"), ne propose plus de menu, FAIS-LE !"""

    def _get_context_prompt(self, app_context: str) -> str:
        """G√©n√®re le contexte sp√©cifique selon l'application"""
        contexts = {
            "cv": """
[CONTEXTE CV] : L'utilisateur est dans Phoenix CV. Tu es son experte en optimisation CV et strat√©gie carri√®re. Focus sur : scores ATS, valorisation comp√©tences, structure CV, keywords sectoriels. Tes r√©ponses doivent √™tre structur√©es avec des m√©triques concr√®tes.""",
            
            "letters": """
[CONTEXTE LETTERS] : L'utilisateur est dans Phoenix Letters. Tu es son experte en lettres de motivation percutantes. Focus sur : personnalisation entreprise, storytelling convaincant, diff√©renciation candidats, analyse offres d'emploi.""",
            
            "website": """
[CONTEXTE WEBSITE] : L'utilisateur est sur le site principal Phoenix. Tu es son guide strat√©gique global. Focus sur : vision carri√®re, choix d'outils, planification parcours, optimisation √©nergie Luna."""
        }
        return contexts.get(app_context, contexts["website"])

    async def _get_user_context_packet(self, user_id: str) -> str:
        """R√©cup√®re le Context Packet structur√© de l'utilisateur"""
        try:
            # G√©n√©ration du Context Packet via Narrative Analyzer
            context_packet = await narrative_analyzer.generate_context_packet(user_id)
            
            # Formatage pour injection dans le prompt Luna
            context_json = json.dumps(context_packet.to_dict(), indent=2, ensure_ascii=False)
            
            return f"""[CONTEXTE NARRATIF STRUCTUR√â]
Donn√©es analytiques utilisateur g√©n√©r√©es par le Narrative Analyzer v1.5 :

{context_json}

INSTRUCTIONS D'USAGE CONTEXTE :
- Si user.plan == "unlimited" ‚Üí Mentionne √©nergie illimit√©e üåô
- Si usage.last_activity_hours > 48 ‚Üí Ton accueillant "Content de te revoir"
- Si progress.ats_delta_pct_14d > 0 ‚Üí F√©licite les progr√®s
- Si last_emotion_or_doubt pr√©sent ‚Üí Aborde subtilement le doute
- Si usage.session_count_7d > 5 ‚Üí Utilisateur motiv√©, propose actions avanc√©es
- Adapte TES suggestions selon progress.letters_target (secteur cibl√©)"""
            
        except Exception as e:
            logger.error("Error generating context packet", user_id=user_id, error=str(e))
            return "[CONTEXTE NARRATIF] : Nouvel utilisateur - Analyse en cours de g√©n√©ration."

    async def generate_response(
        self, 
        user_id: str,
        message: str,
        app_context: str = "website",
        user_name: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        G√©n√®re une r√©ponse Luna avec personnalit√© unifi√©e
        
        Args:
            user_id: ID de l'utilisateur
            message: Message de l'utilisateur
            app_context: Contexte app (cv, letters, website)
            user_name: Pr√©nom utilisateur (optionnel)
        """
        try:
            # 0. üîë S'assurer que Gemini est configur√© avec cl√© API valide
            await self._ensure_genai_configured()
            
            # 1. Construction du prompt unifi√© avec Context Packet + M√âMOIRE + √âTAT
            core_prompt = self._build_luna_core_prompt()
            context_prompt = self._get_context_prompt(app_context)
            context_packet = await self._get_user_context_packet(user_id)
            conversation_memory = await self._get_conversation_memory(user_id)
            conversation_state = await self._get_conversation_state(user_id)
            
            # 2. Assemblage dynamique avec INTELLIGENCE CONVERSATIONNELLE
            state_guidance = self._build_state_guidance(conversation_state)
            
            full_prompt = f"""{core_prompt}

{context_prompt}

{context_packet}

{conversation_memory}

{state_guidance}

# [CONVERSATION ACTUELLE]
{"Utilisateur " + user_name + ": " if user_name else "Utilisateur: "}{message}

Luna, r√©ponds selon ta personnalit√© unifi√©e en tenant compte de ton contexte et du Capital Narratif :"

# [R√âPONSE ATTENDUE]
G√©n√®re une r√©ponse personnalis√©e Luna qui :
- Fait r√©f√©rence √† l'historique si pertinent
- Propose 2-3 actions concr√®tes
- Mentionne les co√ªts √©nerg√©tiques si applicable
- Termine par une question de suivi
"""

            # 3. G√©n√©ration avec Gemini
            response = self.model.generate_content(full_prompt)
            
            if not response or not response.text:
                return {
                    "success": False,
                    "message": "üåô D√©sol√©, j'ai des difficult√©s techniques. Peux-tu reformuler ?",
                    "context": app_context,
                    "energy_consumed": 0,
                    "type": "error"
                }

            # üåô LUNA V2: Classification intelligente conversation vs action
            energy_cost = self._calculate_intelligent_energy_cost(message, response.text.strip())
            
            # üß† SPRINT 2: Sauvegarder la conversation + √âtat pour intelligence
            await self._save_conversation_turn(user_id, message, response.text.strip())
            await self._update_conversation_state(user_id, message, response.text.strip())
            
            return {
                "success": True,
                "message": response.text.strip(),
                "context": app_context,
                "energy_consumed": energy_cost,  # üöÄ Intelligent cost calculation
                "type": "text"
            }
            
        except Exception as e:
            logger.error("Luna Core generation error", user_id=user_id, error=str(e))
            return {
                "success": False,
                "message": "üåô J'ai rencontr√© un probl√®me technique. R√©essaie dans quelques instants !",
                "context": app_context,
                "energy_consumed": 0,
                "type": "error"
            }

# Instance globale (lazy initialization)
luna_core = None

def get_luna_core():
    """R√©cup√®re l'instance Luna Core avec lazy initialization"""
    global luna_core
    if luna_core is None:
        luna_core = LunaCore()
    return luna_core