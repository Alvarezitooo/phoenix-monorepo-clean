"""
üåô Luna AI Service - Service IA pour Phoenix Aube
Clean Architecture - Impl√©mentation Gemini avec Luna Core Personality
"""

import asyncio
import time
from typing import Dict, Any, Optional, List
import logging
import os
import structlog

try:
    import google.generativeai as genai
    GENAI_AVAILABLE = True
except ImportError:
    genai = None
    GENAI_AVAILABLE = False
    logger.warning("‚ö†Ô∏è google-generativeai non disponible - mode d√©grad√©")

try:
    from clients.luna_client import (
        LunaClient, EventRequest, SessionRequest, 
        LunaClientError, NarrativeContext
    )
    LUNA_CLIENT_AVAILABLE = True
except ImportError:
    # Cr√©er des classes placeholder en cas d'import impossible
    class LunaClient: pass
    class EventRequest: pass
    class SessionRequest: pass  
    class LunaClientError(Exception): pass
    class NarrativeContext: pass
    LUNA_CLIENT_AVAILABLE = False
    logger.warning("‚ö†Ô∏è Luna client non disponible - mode d√©grad√©")

logger = structlog.get_logger("aube_gemini_service")

# Configuration Gemini
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
LUNA_HUB_URL = os.getenv("LUNA_HUB_URL", "https://luna-hub-backend-unified-production.up.railway.app")

# Prompts Luna Core pour Phoenix Aube
LUNA_CORE_SYSTEM = """Tu es Luna, un copilote narratif et bienveillant pour la d√©couverte de carri√®re. 
Ta mission est d'aider l'utilisateur √† explorer ses app√©tences professionnelles avec empathie.

üåô PERSONNALIT√â LUNA :
- Ton : Chaleureux, complice, pragmatique
- Style : Phrases courtes, emojis discrets (üéØ, ‚úÖ, üöÄ, üåô)  
- Principe : Suggestions bienveillantes, jamais de verdicts
- Transparence : Toujours expliquer "pourquoi cette question"

üé≠ ADAPTATION PERSONA :
{persona_context}

üìä CONTEXTE UTILISATEUR :
√âtape actuelle : {current_step}
R√©ponses pr√©c√©dentes : {user_signals}
Humeur/√©nergie : {mood_context}

üéØ MISSION :
Analyse les r√©ponses de l'utilisateur et g√©n√®re :
1. Un miroir empathique imm√©diat
2. Des insights sur sa personnalit√© professionnelle  
3. Des recommandations de m√©tiers adapt√©es
4. Un plan de d√©veloppement personnalis√©

Garde toujours la tonalit√© Luna : bienveillante, non-jugeante, encourageante."""

PERSONA_CONTEXTS = {
    "reconversion": """
üéóÔ∏è Persona Reconversion post-burnout/surmenage :
- Posture : Doux, lent, validation de l'effort. Z√©ro injonction.
- √Ä dire : "On reste l√©ger", "sans pression", "√† ton rythme"
- √Ä √©viter : "test", "profilage", "score", urgence
- Focus : Stabilit√©, sens, respect des limites
""",
    "jeune_diplome": """
üß≠ Persona Jeune dipl√¥m√©¬∑e sans cap clair :
- Posture : √ânergisant mais cadr√©, ouvrir les possibles
- √Ä dire : "3 minutes et je te montre 3 m√©tiers qui te ressemblent üöÄ"
- √Ä √©viter : Jargon RH, explications lourdes
- Focus : Potentiel, d√©couverte, premi√®res exp√©riences
""",
    "pivot_tech": """  
üß© Persona Pivot Tech ‚Üí Design/Produit :
- Posture : Rassurer sur la transf√©rabilit√© des comp√©tences
- √Ä dire : "Tes skills tech sont un atout, pas un frein"
- Focus : Bridge skills, √©volution naturelle, capitalisation
""",
    "ops_data": """
üóÇÔ∏è Persona Ops ‚Üí Data/Analyse :
- Posture : Version no-code d'abord, technique progressive
- √Ä dire : "Les Ops comprennent les syst√®mes, la data va te parler"
- Focus : Transition douce, process thinking, analyse
""",
    "reprise": """
üë∂ Persona Reprise apr√®s pause (parentalit√©, etc.) :
- Posture : Int√©grer les contraintes d√®s le d√©part  
- √Ä dire : "On adapte tout √† ta situation, c'est ton cadre de vie"
- Focus : Flexibilit√©, contraintes r√©elles, √©quilibre
"""
}

class LunaGeminiService:
    """Service IA Luna pour Phoenix Aube avec Gemini + Luna Hub Memory"""
    
    def __init__(self, api_key: Optional[str] = None, model_name: str = "gemini-1.5-flash", luna_client: Optional[LunaClient] = None):
        self.api_key = api_key or GOOGLE_API_KEY
        self.model_name = model_name
        self.model = None
        self._is_configured = False
        self.luna_client = luna_client
        
        if self.api_key and GENAI_AVAILABLE:
            self._configure_client()
        else:
            if not GENAI_AVAILABLE:
                logger.warning("‚ö†Ô∏è google-generativeai non disponible - Luna utilisera des r√©ponses pr√©-d√©finies")
            elif not self.api_key:
                logger.warning("‚ö†Ô∏è GOOGLE_API_KEY manquant - Luna utilisera des r√©ponses pr√©-d√©finies")
    
    def _configure_client(self) -> None:
        """Configuration du client Gemini pour Luna"""
        try:
            if not GENAI_AVAILABLE:
                raise ImportError("google-generativeai not available")
            genai.configure(api_key=self.api_key)
            self.model = genai.GenerativeModel(
                model_name=self.model_name,
                generation_config={
                    "temperature": 0.7,  # Cr√©ativit√© mod√©r√©e pour Luna
                    "top_p": 0.8,
                    "top_k": 40,
                    "max_output_tokens": 1024,
                }
            )
            self._is_configured = True
            logger.info("‚úÖ Luna Gemini Service configur√©", model=self.model_name)
        except Exception as e:
            logger.error("‚ùå Configuration Luna Gemini √©chou√©e", error=str(e))
            self._is_configured = False

    async def luna_mirror_response(
        self, 
        user_response: str, 
        persona: str = "jeune_diplome",
        context: Dict[str, Any] = None,
        user_id: Optional[str] = None
    ) -> str:
        """
        üåô G√©n√®re un miroir empathique imm√©diat de Luna
        
        Args:
            user_response: R√©ponse de l'utilisateur
            persona: Type de persona (reconversion, jeune_diplome, etc.)
            context: Contexte additionnel (√©tape, signaux pr√©c√©dents)
        
        Returns:
            Miroir empathique personnalis√©
        """
        if not self._is_configured:
            # Fallback sur r√©ponses pr√©-d√©finies
            return self._fallback_mirror_response(user_response, persona)
        
        try:
            context = context or {}
            persona_context = PERSONA_CONTEXTS.get(persona, PERSONA_CONTEXTS["jeune_diplome"])
            
            # üß† R√©cup√©ration Capital Narratif Luna Hub
            narrative_context = ""
            if user_id and self.luna_client:
                try:
                    narrative = self.luna_client.get_narrative_context(user_id)
                    if narrative:
                        narrative_context = f"""
üß† M√âMOIRE LUNA (contexte utilisateur) :
- Profil : {narrative.user_profile.get('skills_demonstrated', [])}
- Parcours : {narrative.professional_journey.get('career_progression', [])}
- Insights pr√©c√©dents : {narrative.ai_insights.get('recommendations', [])}
- Engagement : {narrative.ai_insights.get('engagement_score', 'nouveau')}
                        """
                except Exception as e:
                    logger.warning("Capital narratif indisponible", error=str(e))
            
            prompt = f"""{LUNA_CORE_SYSTEM.format(
                persona_context=persona_context,
                current_step=context.get('step', 'assessment'),
                user_signals=context.get('signals', {}),
                mood_context=context.get('mood', 'neutre')
            )}
            
{narrative_context}

R√âPONSE UTILISATEUR √Ä ANALYSER :
"{user_response}"

G√©n√®re un MIROIR EMPATHIQUE court (1-2 phrases max) qui :
- Valide ce que tu entends 
- Montre que tu comprends
- Donne un insight bref
- Reste dans la tonalit√© {persona}

Exemple format : "Merci üôè J'entends que tu valorises [insight]. Je note √ßa pour tes pistes m√©tiers ‚ú®"
"""

            response = await self.model.generate_content_async(prompt)
            luna_response = response.text.strip()
            
            # üìä Event Sourcing - Tracker l'interaction Luna
            if user_id and self.luna_client:
                try:
                    await asyncio.get_event_loop().run_in_executor(
                        None,
                        self.luna_client.track_event,
                        EventRequest(
                            user_id=user_id,
                            event_type="luna_mirror_generated",
                            event_data={
                                "persona": persona,
                                "user_response_length": len(user_response),
                                "luna_response_length": len(luna_response),
                                "step": context.get('step', 'unknown')
                            }
                        )
                    )
                except Exception as e:
                    logger.warning("Event tracking failed", error=str(e))
            
            logger.info("üåô Luna mirror g√©n√©r√©", 
                       persona=persona, 
                       user_input_length=len(user_response),
                       luna_response_length=len(luna_response))
            
            return luna_response
            
        except Exception as e:
            logger.error("‚ùå Erreur g√©n√©ration Luna mirror", error=str(e))
            return self._fallback_mirror_response(user_response, persona)

    async def luna_career_analysis(
        self,
        user_signals: Dict[str, Any],
        persona: str = "jeune_diplome",
        depth: str = "ultra_light",  # ultra_light, court, profond
        user_id: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        üåô Analyse compl√®te Luna avec recommandations m√©tiers
        
        Args:
            user_signals: Toutes les r√©ponses de l'utilisateur
            persona: Type de persona 
            depth: Profondeur d'analyse (ultra_light, court, profond)
        
        Returns:
            Analyse compl√®te avec m√©tiers recommand√©s
        """
        if not self._is_configured:
            return self._fallback_career_analysis(user_signals, persona, depth)
        
        try:
            persona_context = PERSONA_CONTEXTS.get(persona, PERSONA_CONTEXTS["jeune_diplome"])
            
            # üß† R√©cup√©ration Capital Narratif pour analyse enrichie
            narrative_context = ""
            if user_id and self.luna_client:
                try:
                    narrative = self.luna_client.get_narrative_context(user_id)
                    if narrative:
                        narrative_context = f"""
üß† HISTORIQUE UTILISATEUR LUNA HUB :
- Applications ma√Ætris√©es : {narrative.user_profile.get('apps_mastered', [])}
- Comp√©tences d√©montr√©es : {narrative.user_profile.get('skills_demonstrated', [])}
- Progression carri√®re : {narrative.professional_journey.get('career_progression', [])}
- Actions fr√©quentes : {narrative.usage_analytics.get('actions_breakdown', {})}
- Score engagement : {narrative.ai_insights.get('engagement_score', 'nouveau')}
- Candidat upgrade : {narrative.usage_analytics.get('subscription_insights', {}).get('upgrade_candidate', False)}

‚ö° UTILISE CES DONN√âES pour personnaliser les recommandations !
                        """
                except Exception as e:
                    logger.warning("Capital narratif indisponible", error=str(e))
            
            # Prompt adapt√© selon la profondeur
            depth_instructions = {
                "ultra_light": "Top 3 m√©tiers avec raisons courtes",
                "court": "Top 5 m√©tiers + plan IA + timeline 6 mois", 
                "profond": "Top 5 m√©tiers d√©taill√©s + plan IA complet + timeline 12 mois + skills bridge"
            }
            
            prompt = f"""{LUNA_CORE_SYSTEM.format(
                persona_context=persona_context,
                current_step="career_analysis",
                user_signals=user_signals,
                mood_context=user_signals.get('mood', 'neutre')
            )}

{narrative_context}

MISSION ANALYSE {depth.upper()} :
{depth_instructions[depth]}

SIGNAUX UTILISATEUR COMPLETS :
{user_signals}

G√©n√®re une analyse JSON avec cette structure :
{{
    "luna_insights": "Ton analyse empathique en 2-3 phrases Luna",
    "career_matches": [
        {{
            "title": "Nom du m√©tier",
            "compatibility_score": 0.85,
            "luna_reasoning": "Pourquoi ce m√©tier lui correspond (style Luna)",
            "future_proof_score": 0.9,
            "salary_range": "45k-70k ‚Ç¨",
            "transition_difficulty": "facile|mod√©r√©|√©lev√©"
        }}
    ],
    "next_steps": [
        "Action concr√®te 1 recommand√©e par Luna",
        "Action concr√®te 2"
    ],
    "luna_encouragement": "Message encourageant final style Luna"
}}

Reste dans la tonalit√© {persona} et utilise des insights psychologiques fins.
"""

            response = await self.model.generate_content_async(prompt)
            
            # Parse JSON response
            import json
            try:
                analysis = json.loads(response.text.strip())
                
                # üìä Event Sourcing - Tracker l'analyse carri√®re
                if user_id and self.luna_client:
                    try:
                        await asyncio.get_event_loop().run_in_executor(
                            None,
                            self.luna_client.track_event,
                            EventRequest(
                                user_id=user_id,
                                event_type="luna_career_analysis_generated",
                                event_data={
                                    "persona": persona,
                                    "depth": depth,
                                    "career_matches_count": len(analysis.get('career_matches', [])),
                                    "has_narrative_context": bool(narrative_context)
                                }
                            )
                        )
                    except Exception as e:
                        logger.warning("Event tracking failed", error=str(e))
                
                logger.info("üåô Luna analysis g√©n√©r√©e", 
                           persona=persona,
                           depth=depth,
                           career_matches_count=len(analysis.get('career_matches', [])))
                
                return analysis
                
            except json.JSONDecodeError:
                logger.warning("‚ö†Ô∏è Luna response n'est pas du JSON valide, fallback")
                return self._fallback_career_analysis(user_signals, persona, depth)
                
        except Exception as e:
            logger.error("‚ùå Erreur Luna career analysis", error=str(e))
            return self._fallback_career_analysis(user_signals, persona, depth)

    def _fallback_mirror_response(self, user_response: str, persona: str) -> str:
        """R√©ponses miroir pr√©-d√©finies si Gemini indisponible"""
        mirrors = {
            "reconversion": [
                "Merci üôè J'entends tes priorit√©s. On va chercher en douceur.",
                "Ok, je note ces √©l√©ments importants pour toi. On avance √† ton rythme ‚ú®",
                "Je respecte ces choix. Ils vont guider mes suggestions üåô"
            ],
            "jeune_diplome": [
                "Int√©ressant ! Je vois des pistes qui √©mergent üöÄ", 
                "Cool ! √áa m'aide √† cerner ton profil. On continue ?",
                "Merci ! Ces infos vont affiner tes recommandations m√©tiers ‚ú®"
            ]
        }
        
        persona_mirrors = mirrors.get(persona, mirrors["jeune_diplome"])
        import random
        return random.choice(persona_mirrors)

    def _fallback_career_analysis(self, signals: Dict, persona: str, depth: str) -> Dict[str, Any]:
        """Analyse fallback bas√©e sur r√®gles simples si Gemini indisponible"""
        # Logique simplifi√©e bas√©e sur les signaux
        duos = signals.get('duos', {})
        territories = signals.get('territories', [])
        
        careers = []
        if duos.get('people_data') == 'people' and 'design_humain' in territories:
            careers.extend(['UX Designer', 'Product Manager', 'Service Designer'])
        if duos.get('people_data') == 'data' and 'produit_data' in territories:
            careers.extend(['Data Analyst', 'Business Intelligence', 'Product Analyst'])
        
        if not careers:
            careers = ['Product Manager', 'UX Designer', 'Data Analyst']
        
        return {
            "luna_insights": f"D'apr√®s tes r√©ponses, tu as un profil {persona} avec des app√©tences vari√©es üåô",
            "career_matches": [
                {
                    "title": career,
                    "compatibility_score": 0.8,
                    "luna_reasoning": f"Ce m√©tier correspond √† tes choix dans l'exercice",
                    "future_proof_score": 0.85,
                    "salary_range": "45k-70k ‚Ç¨",
                    "transition_difficulty": "mod√©r√©"
                } for career in careers[:3]
            ],
            "next_steps": [
                "Explorer ces m√©tiers plus en d√©tail",
                "Faire un assessment complet pour affiner"
            ],
            "luna_encouragement": "Tu as des pistes solides ! On peut creuser ensemble si tu veux üöÄ"
        }

# Fonction pour cr√©er un client Luna configur√©
def create_luna_client() -> Optional[LunaClient]:
    """Cr√©er client Luna Hub avec token provider"""
    try:
        if not LUNA_CLIENT_AVAILABLE:
            return None
            
        def token_provider():
            # En production, r√©cup√©rer depuis la session/header
            # Pour l'instant, on peut utiliser un token de service
            return os.getenv("LUNA_SERVICE_TOKEN", "")
        
        return LunaClient(token_provider=token_provider)
    except Exception as e:
        logger.warning("Luna client unavailable", error=str(e))
        return None

# Instance globale du service Luna avec client
_luna_client = create_luna_client()
luna_service = LunaGeminiService(luna_client=_luna_client)